:py:mod:`anomalib.pre_processing.tiler`
=======================================

.. py:module:: anomalib.pre_processing.tiler

.. autoapi-nested-parse::

   Image Tiler.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   anomalib.pre_processing.tiler.Tiler



Functions
~~~~~~~~~

.. autoapisummary::

   anomalib.pre_processing.tiler.compute_new_image_size
   anomalib.pre_processing.tiler.upscale_image
   anomalib.pre_processing.tiler.downscale_image



.. py:exception:: StrideSizeError

   Bases: :py:obj:`Exception`

   StrideSizeError to raise exception when stride size is greater than the tile size.


.. py:function:: compute_new_image_size(image_size: Tuple, tile_size: Tuple, stride: Tuple) -> Tuple

   This function checks if image size is divisible by tile size and stride.

   If not divisible, it resizes the image size to make it divisible.

   :param image_size: Original image size
   :type image_size: Tuple
   :param tile_size: Tile size
   :type tile_size: Tuple
   :param stride: Stride
   :type stride: Tuple

   .. rubric:: Examples

   >>> compute_new_image_size(image_size=(512, 512), tile_size=(256, 256), stride=(128, 128))
   (512, 512)

   >>> compute_new_image_size(image_size=(512, 512), tile_size=(222, 222), stride=(111, 111))
   (555, 555)

   :returns: Updated image size that is divisible by tile size and stride.
   :rtype: Tuple


.. py:function:: upscale_image(image: torch.Tensor, size: Tuple, mode: str = 'padding') -> torch.Tensor

   Upscale image to the desired size via either padding or interpolation.

   :param image: Image
   :type image: Tensor
   :param size: Tuple to which image is upscaled.
   :type size: Tuple
   :param mode: Upscaling mode. Defaults to "padding".
   :type mode: str, optional

   .. rubric:: Examples

   >>> image = torch.rand(1, 3, 512, 512)
   >>> image = upscale_image(image, size=(555, 555), mode="padding")
   >>> image.shape
   torch.Size([1, 3, 555, 555])

   >>> image = torch.rand(1, 3, 512, 512)
   >>> image = upscale_image(image, size=(555, 555), mode="interpolation")
   >>> image.shape
   torch.Size([1, 3, 555, 555])

   :returns: Upscaled image.
   :rtype: Tensor


.. py:function:: downscale_image(image: torch.Tensor, size: Tuple, mode: str = 'padding') -> torch.Tensor

   Opposite of upscaling. This image downscales image to a desired size.

   :param image: Input image
   :type image: Tensor
   :param size: Size to which image is down scaled.
   :type size: Tuple
   :param mode: Downscaling mode. Defaults to "padding".
   :type mode: str, optional

   .. rubric:: Examples

   >>> x = torch.rand(1, 3, 512, 512)
   >>> y = upscale_image(image, upscale_size=(555, 555), mode="padding")
   >>> y = downscale_image(y, size=(512, 512), mode='padding')
   >>> torch.allclose(x, y)
   True

   :returns: Downscaled image
   :rtype: Tensor


.. py:class:: Tiler(tile_size: Union[int, Sequence], stride: Union[int, Sequence], remove_border_count: int = 0, mode: str = 'padding', tile_count: int = 4)

   Tile Image into (non)overlapping Patches. Images are tiled in order to efficiently process large images.

   :param tile_size: Tile dimension for each patch
   :param stride: Stride length between patches
   :param remove_border_count: Number of border pixels to be removed from tile before untiling
   :param mode: Upscaling mode for image resize.Supported formats: padding, interpolation

   .. rubric:: Examples

   >>> import torch
   >>> from torchvision import transforms
   >>> from skimage.data import camera
   >>> tiler = Tiler(tile_size=256,stride=128)
   >>> image = transforms.ToTensor()(camera())
   >>> tiles = tiler.tile(image)
   >>> image.shape, tiles.shape
   (torch.Size([3, 512, 512]), torch.Size([9, 3, 256, 256]))

   >>> # Perform your operations on the tiles.

   >>> # Untile the patches to reconstruct the image
   >>> reconstructed_image = tiler.untile(tiles)
   >>> reconstructed_image.shape
   torch.Size([1, 3, 512, 512])

   .. py:method:: __validate_size_type(parameter: Union[int, Sequence]) -> Tuple[int, Ellipsis]
      :staticmethod:


   .. py:method:: __random_tile(self, image: torch.Tensor) -> torch.Tensor

      Randomly crop tiles from the given image.

      :param image: input image to be cropped

      Returns: Randomly cropped tiles from the image


   .. py:method:: __unfold(self, tensor: torch.Tensor) -> torch.Tensor

      Unfolds tensor into tiles.

      This is the core function to perform tiling operation.

      :param tensor: Input tensor from which tiles are generated.

      Returns: Generated tiles


   .. py:method:: __fold(self, tiles: torch.Tensor) -> torch.Tensor

      Fold the tiles back into the original tensor.

      This is the core method to reconstruct the original image from its tiled version.

      :param tiles: Tiles from the input image, generated via __unfold method.

      :returns: Output that is the reconstructed version of the input tensor.


   .. py:method:: tile(self, image: torch.Tensor, use_random_tiling: Optional[bool] = False) -> torch.Tensor

      Tiles an input image to either overlapping, non-overlapping or random patches.

      :param image: Input image to tile.

      .. rubric:: Examples

      >>> from anomalib.data.tiler import Tiler
      >>> tiler = Tiler(tile_size=512,stride=256)
      >>> image = torch.rand(size=(2, 3, 1024, 1024))
      >>> image.shape
      torch.Size([2, 3, 1024, 1024])
      >>> tiles = tiler.tile(image)
      >>> tiles.shape
      torch.Size([18, 3, 512, 512])

      :returns: Tiles generated from the image.


   .. py:method:: untile(self, tiles: torch.Tensor) -> torch.Tensor

      Untiles patches to reconstruct the original input image.

      If patches, are overlapping patches, the function averages the overlapping pixels,
      and return the reconstructed image.

      :param tiles: Tiles from the input image, generated via tile()..

      .. rubric:: Examples

      >>> from anomalib.datasets.tiler import Tiler
      >>> tiler = Tiler(tile_size=512,stride=256)
      >>> image = torch.rand(size=(2, 3, 1024, 1024))
      >>> image.shape
      torch.Size([2, 3, 1024, 1024])
      >>> tiles = tiler.tile(image)
      >>> tiles.shape
      torch.Size([18, 3, 512, 512])
      >>> reconstructed_image = tiler.untile(tiles)
      >>> reconstructed_image.shape
      torch.Size([2, 3, 1024, 1024])
      >>> torch.equal(image, reconstructed_image)
      True

      :returns: Output that is the reconstructed version of the input tensor.



