:py:mod:`anomalib.utils.sweep.helpers.inference`
================================================

.. py:module:: anomalib.utils.sweep.helpers.inference

.. autoapi-nested-parse::

   Utils to help compute inference statistics.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   anomalib.utils.sweep.helpers.inference.MockImageLoader



Functions
~~~~~~~~~

.. autoapisummary::

   anomalib.utils.sweep.helpers.inference.get_meta_data
   anomalib.utils.sweep.helpers.inference.get_torch_throughput
   anomalib.utils.sweep.helpers.inference.get_openvino_throughput



.. py:class:: MockImageLoader(image_size: List[int], total_count: int)

   Create mock images for inference on CPU based on the specifics of the original torch test dataset.

   Uses yield so as to avoid storing everything in the memory.

   :param image_size: Size of input image
   :type image_size: List[int]
   :param total_count: Total images in the test dataset
   :type total_count: int

   .. py:method:: __len__(self)

      Get total count of images.


   .. py:method:: __call__(self) -> Iterable[numpy.ndarray]

      Yield batch of generated images.

      :param idx: Unused
      :type idx: int



.. py:function:: get_meta_data(model: anomalib.models.components.AnomalyModule, input_size: Tuple[int, int]) -> Dict

   Get meta data for inference.

   :param model: Trained model from which the metadata is extracted.
   :type model: AnomalyModule
   :param input_size: Input size used to resize the pixel level mean and std.
   :type input_size: Tuple[int, int]

   :returns: Metadata as dictionary.
   :rtype: (Dict)


.. py:function:: get_torch_throughput(config: Union[omegaconf.DictConfig, omegaconf.ListConfig], model: anomalib.models.components.AnomalyModule, test_dataset: torch.utils.data.DataLoader, meta_data: Dict) -> float

   Tests the model on dummy data. Images are passed sequentially to make the comparision with OpenVINO model fair.

   :param config: Model config.
   :type config: Union[DictConfig, ListConfig]
   :param model: Model on which inference is called.
   :type model: Path
   :param test_dataset: The test dataset used as a reference for the mock dataset.
   :type test_dataset: DataLoader
   :param meta_data: Metadata used for normalization.
   :type meta_data: Dict

   :returns: Inference throughput
   :rtype: float


.. py:function:: get_openvino_throughput(config: Union[omegaconf.DictConfig, omegaconf.ListConfig], model_path: pathlib.Path, test_dataset: torch.utils.data.DataLoader, meta_data: Dict) -> float

   Runs the generated OpenVINO model on a dummy dataset to get throughput.

   :param config: Model config.
   :type config: Union[DictConfig, ListConfig]
   :param model_path: Path to folder containing the OpenVINO models. It then searches `model.xml` in the folder.
   :type model_path: Path
   :param test_dataset: The test dataset used as a reference for the mock dataset.
   :type test_dataset: DataLoader
   :param meta_data: Metadata used for normalization.
   :type meta_data: Dict

   :returns: Inference throughput
   :rtype: float


